{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP243FinalProject.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NpT-wD2NugSG","executionInfo":{"status":"ok","timestamp":1638405926538,"user_tz":480,"elapsed":7843,"user":{"displayName":"Likith Boddeda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15360136892021079586"}},"outputId":"fc1bb2b9-b3e4-4235-8b7b-9126b9a3e9cf"},"source":["#imports\n","#make sure to rerun this cell continually \n","import nltk\n","from nltk.probability import FreqDist\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.multiclass import OneVsRestClassifier\n","from sklearn.linear_model import SGDClassifier\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch\n","import torch.nn.functional as F\n","#example text"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"markdown","metadata":{"id":"7KJPdm8pvr0Q"},"source":["#Downloading Data\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"ZlDH4rqFwMl0","executionInfo":{"status":"ok","timestamp":1638405926545,"user_tz":480,"elapsed":35,"user":{"displayName":"Likith Boddeda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15360136892021079586"}},"outputId":"a1bf61ab-45b9-49d6-fdcd-089232ae3429"},"source":["#download data(onestop english)(Julian)\n","onestop_data = {'Advanced':[], 'Intermediate':[]}\n","with open('ADV-INT.txt', 'r') as file:\n","  adv_int_lines = file.readlines()\n","  #print(adv_int_data[0])\n","  for i in range(0, len(adv_int_lines), 3):\n","    onestop_data['Advanced'].append(adv_int_lines[i])\n","    onestop_data['Intermediate'].append(adv_int_lines[i+1])\n","onestop_df = pd.DataFrame(onestop_data)\n","onestop_df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Advanced</th>\n","      <th>Intermediate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Brazil and Peru have lodged objections to a bi...</td>\n","      <td>Brazil and Peru have made objections to a bid ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Until now, the differences between commercial,...</td>\n","      <td>Until now, the differences between commercial,...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>But these categories  or generic top-level dom...</td>\n","      <td>But these categories  or generic top-level dom...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Amazon has applied for dozens of new domains, ...</td>\n","      <td>Amazon has applied for many new domains, inclu...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Allowing private companies to register geograp...</td>\n","      <td>Allowing private companies to register geograp...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2149</th>\n","      <td>Workers on zero-hours contracts are often only...</td>\n","      <td>Workers on zero-hours contracts are often only...</td>\n","    </tr>\n","    <tr>\n","      <th>2150</th>\n","      <td>We believe zero-hours contracts are essential ...</td>\n","      <td>We believe zero-hours contracts are essential ...</td>\n","    </tr>\n","    <tr>\n","      <th>2151</th>\n","      <td>Our properties have told us its important to b...</td>\n","      <td>Its important to be able to reorganize staff r...</td>\n","    </tr>\n","    <tr>\n","      <th>2152</th>\n","      <td>The institutes  gures also suggest that 17% of...</td>\n","      <td>Figures from the poll suggest that 17% of empl...</td>\n","    </tr>\n","    <tr>\n","      <th>2153</th>\n","      <td>Industries where employers were most likely to...</td>\n","      <td>Industries where employers were most likely to...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2154 rows × 2 columns</p>\n","</div>"],"text/plain":["                                               Advanced                                       Intermediate\n","0     Brazil and Peru have lodged objections to a bi...  Brazil and Peru have made objections to a bid ...\n","1     Until now, the differences between commercial,...  Until now, the differences between commercial,...\n","2     But these categories  or generic top-level dom...  But these categories  or generic top-level dom...\n","3     Amazon has applied for dozens of new domains, ...  Amazon has applied for many new domains, inclu...\n","4     Allowing private companies to register geograp...  Allowing private companies to register geograp...\n","...                                                 ...                                                ...\n","2149  Workers on zero-hours contracts are often only...  Workers on zero-hours contracts are often only...\n","2150  We believe zero-hours contracts are essential ...  We believe zero-hours contracts are essential ...\n","2151  Our properties have told us its important to b...  Its important to be able to reorganize staff r...\n","2152  The institutes  gures also suggest that 17% of...  Figures from the poll suggest that 17% of empl...\n","2153  Industries where employers were most likely to...  Industries where employers were most likely to...\n","\n","[2154 rows x 2 columns]"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"2-fZZvbiwa1Y","executionInfo":{"status":"ok","timestamp":1638405929166,"user_tz":480,"elapsed":318,"user":{"displayName":"Likith Boddeda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15360136892021079586"}},"outputId":"f4e61ae3-64e1-480b-aa5c-bc94cb74734c"},"source":["#download data (wiki manual)(Karl)\n","#initial import is dev data (4mb), secondd version is for the much larger train data(114 mb)\n","wiki_df = pd.read_csv(\"valid.tsv\", sep = \"\\t\", header=0)\n","#wiki_df = pd.read_csv(\"train.tsv\", sep = \"\\t\", header=0)\n","wiki_df\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Advanced</th>\n","      <th>Intermediate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Adjacent counties are Marin (to the south), Me...</td>\n","      <td>countries next to it are Marin, Mendocino, Lak...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Adjacent counties are Marin (to the south), Me...</td>\n","      <td>Nearby counties are Marin, Mendocino, Lake, Na...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Adjacent counties are Marin (to the south), Me...</td>\n","      <td>Adjacent counties are Marin, Mendocino, Lake, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Adjacent counties are Marin (to the south), Me...</td>\n","      <td>Neighboring counties are Marin, Mendocino, Lak...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Adjacent counties are Marin (to the south), Me...</td>\n","      <td>Adjacent counties are Marin (south), Mendocino...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>19995</th>\n","      <td>Modern African history has been rife with revo...</td>\n","      <td>Modern African history is full of revolutions,...</td>\n","    </tr>\n","    <tr>\n","      <th>19996</th>\n","      <td>Modern African history has been rife with revo...</td>\n","      <td>Modern African history is full of revolutions ...</td>\n","    </tr>\n","    <tr>\n","      <th>19997</th>\n","      <td>Modern African history has been rife with revo...</td>\n","      <td>Common in modern African history are wars and ...</td>\n","    </tr>\n","    <tr>\n","      <th>19998</th>\n","      <td>Modern African history has been rife with revo...</td>\n","      <td>Revolutions and wars, and the growth of econom...</td>\n","    </tr>\n","    <tr>\n","      <th>19999</th>\n","      <td>Modern African history has been rife with revo...</td>\n","      <td>Modern African history is common with revoluti...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>20000 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                Advanced                                       Intermediate\n","0      Adjacent counties are Marin (to the south), Me...  countries next to it are Marin, Mendocino, Lak...\n","1      Adjacent counties are Marin (to the south), Me...  Nearby counties are Marin, Mendocino, Lake, Na...\n","2      Adjacent counties are Marin (to the south), Me...  Adjacent counties are Marin, Mendocino, Lake, ...\n","3      Adjacent counties are Marin (to the south), Me...  Neighboring counties are Marin, Mendocino, Lak...\n","4      Adjacent counties are Marin (to the south), Me...  Adjacent counties are Marin (south), Mendocino...\n","...                                                  ...                                                ...\n","19995  Modern African history has been rife with revo...  Modern African history is full of revolutions,...\n","19996  Modern African history has been rife with revo...  Modern African history is full of revolutions ...\n","19997  Modern African history has been rife with revo...  Common in modern African history are wars and ...\n","19998  Modern African history has been rife with revo...  Revolutions and wars, and the growth of econom...\n","19999  Modern African history has been rife with revo...  Modern African history is common with revoluti...\n","\n","[20000 rows x 2 columns]"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"k6Xl9Ok3yYcc","colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"status":"error","timestamp":1638405932355,"user_tz":480,"elapsed":554,"user":{"displayName":"Likith Boddeda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15360136892021079586"}},"outputId":"94f10005-6b98-4aec-d991-a004aeae7f92"},"source":["#dowload embeddings for glove or bert (Karl)\n","#glove embeddings:\n","embeddings_dict = {}\n","with open(\"glove.6B.100d.txt\", 'r', encoding=\"utf-8\") as f:\n","  for line in f:\n","    values = line.split()\n","    word = values[0]\n","    vector = np.asarray(values[1:], \"float32\")\n","    embeddings_dict[word] = vector"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-b6fdad62c64c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#glove embeddings:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0membeddings_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"glove.6B.100d.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.100d.txt'"]}]},{"cell_type":"markdown","metadata":{"id":"qcj59bElxVPE"},"source":["#Data Wrangling"]},{"cell_type":"code","metadata":{"id":"FLMvMGDIwyrs"},"source":["stop_words = set(stopwords.words('english'))\n","def rm_stopwords(sentence):\n","  filtered_sentence = []\n","  for word in sentence:\n","    if word.lower() not in stop_words:\n","      filtered_sentence.append(word)\n","  return filtered_sentence\n","\n","def split_data(df, split=0.1):\n","    dev_set = df.sample(frac = split, random_state = 7)\n","    dev_set.reset_index(inplace=True, drop=True)\n","    train_set = df.drop(dev_set.index)\n","    train_set.reset_index(inplace=True, drop=True)\n","    return train_set, dev_set\n","\n","#preprocessing data data to pandas or something to this tune(Julian)\n","onestop_df['Advanced'] = onestop_df['Advanced'].apply(word_tokenize)\n","onestop_df['Intermediate'] = onestop_df['Intermediate'].apply(word_tokenize)\n","#onestop_df['Advanced'] = onestop_df['Advanced'].apply(rm_stopwords)\n","#onestop_df['Intermediate'] = onestop_df['Intermediate'].apply(rm_stopwords)\n","onestop_df\n","\n","training_data = pd.concat(pd.DataFrame([[row['Advanced'], 'Advanced'],\n","                                        [row['Intermediate'], 'Intermediate']],\n","                                        columns=['Sentences', 'Tag']) for index, row in onestop_df.iterrows())\n","training_data.reset_index(inplace=True, drop=True)\n","train, dev = split_data(training_data)\n","bla = training_data.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RSGdKMPsxaIk","executionInfo":{"status":"ok","timestamp":1638404699787,"user_tz":480,"elapsed":585,"user":{"displayName":"Julian Cremer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16604329281986994206"}},"outputId":"8178604c-184b-4da4-e3b4-36ca2b6701cc"},"source":["#data viewing (checking shape that sort of thing)(Julian)\n","onestop_adv_vocab = []\n","onestop_int_vocab = []\n","onestop_int_avg_len = 0\n","onestop_adv_avg_len = 0\n","for index, row in onestop_df.iterrows():\n","  onestop_int_avg_len += len(row['Intermediate'])\n","  onestop_adv_avg_len += len(row['Advanced'])\n","  for word in row['Advanced']:\n","    onestop_adv_vocab.append(word)\n","  for word in row['Intermediate']:\n","    onestop_int_vocab.append(word)\n","onestop_int_avg_len = onestop_int_avg_len/len(onestop_df)\n","print('The average length of an Intermediate sentence is:', onestop_int_avg_len)\n","onestop_adv_avg_len = onestop_adv_avg_len/len(onestop_df)\n","print('The average length of an Advanced sentence is:', onestop_adv_avg_len)\n","onestop_adv_fdist = FreqDist(onestop_adv_vocab)\n","print('Advanced Sentence Vocab Size:', len(onestop_adv_fdist))\n","onestop_int_fdist = FreqDist(onestop_int_vocab)\n","print('Intermediate Sentence Vocab Size:', len(onestop_int_fdist))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The average length of an Intermediate sentence is: 26.402042711234913\n","The average length of an Advanced sentence is: 28.41643454038997\n","Advanced Sentence Vocab Size: 10711\n","Intermediate Sentence Vocab Size: 9293\n"]}]},{"cell_type":"code","metadata":{"id":"Rane5WtUyhQJ"},"source":["#glove embeddings config (Karl)\n","#try both 50 and 100 dimm\n","#embeddings \n","#target_vocab needs to be defined dictionary of the vocab\n","#target_vocab = onestop_adv_fdist | onestop_int_fdist\n","def glove_embedding(target_vocab, embeddings_dict):\n","  embedding_dimm = 100 # change this value as file is changed\n","  matrix_len = len(target_vocab)\n","  weights_matrix = np.zeros((matrix_len, embedding_dimm))\n","  words_found = 0\n","\n","  for i, word in enumerate(target_vocab):\n","      try: \n","          weights_matrix[i] = embeddings_dict[word]\n","          words_found += 1\n","      except KeyError:\n","        #continue\n","        weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dimm, ))\n","  print (weights_matrix.shape)\n","  return weights_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KgAwwKQByjPq"},"source":["#bert implementation (implement later)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nGU59yWKVqPA"},"source":["#data pre-process for baseline classifier\n","\n","corpus = wiki_df\n","#make it so models can acutally intake the data\n","#x = [label.split() for label in corpus['Advanced']] + [label.split() for label in corpus['Intermediate']]\n","x = pd.concat([corpus['Advanced'], corpus['Intermediate']])\n","#print ((x))\n","y = [1] * len(corpus) + [0]*len(corpus)\n","#print (y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wYrWiQyH94hU"},"source":["# data preproccessing functions for lstm classifiers\n","def get_ix_converter(sentences):\n","  word_to_ix = dict()\n","  for sentence in sentences:\n","    for word in sentence:\n","      if word not in word_to_ix:\n","        word_to_ix[word] = len(word_to_ix)\n","  return word_to_ix\n","\n","def prepare_sequence(sentence, to_ix, w_unk=False):\n","  if w_unk:\n","    idxs = [to_ix.get(word, to_ix['<UNK>']) for word in sentence]\n","  else: \n","    idxs = [to_ix[word] for word in sentence]\n","  return torch.tensor(idxs, dtype=torch.long)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X2jB3F7QxBdp"},"source":["#Classifier"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"btjuUiikyMwn","executionInfo":{"status":"ok","timestamp":1638404701455,"user_tz":480,"elapsed":1676,"user":{"displayName":"Julian Cremer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16604329281986994206"}},"outputId":"fb2e7963-a591-444c-9fec-52ba6b51ff8b"},"source":["#baseline non neural model uses tfidf vectorization can modify to use word embedding(SGD)(Karl)\n","#binarizer = MultiLabelBinarizer()\n","#binarizer.fit(y)\n","#y = binarizer.transform(y)\n","\n","x_train, x_dt, y_train, y_dt = train_test_split(x, y, test_size = .3, random_state = 0)\n","#x_dev, x_test, y_dev, y_test = train_test_split(x_dt, y_dt, test_dt = .5, random_state = 0)\n","tfidf = TfidfVectorizer(min_df = 0)\n","print(type(x_train))\n","fitted = tfidf.fit(x_train.append(x_dt))\n","x_train = fitted.transform(x_train)\n","x_dt = fitted.transform(x_dt)\n","print(x_train.shape)\n","#train_data.describe()\n","\n","classifier = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha =.0001))\n","classifier.fit(x_train, y_train)\n","\n","\n","y_pred = classifier.predict(x_dt)\n","count = 0\n","right = 0\n","print (y_pred)\n","#y_pred = binarizer.inverse_transform(y_pred)\n","#y_dt = binarizer.inverse_transform(y_dt)\n","for i in range(0,len(y_pred)):\n","  if y_pred[i] == ():\n","    y_pred[i] = ('none',)\n","  if y_pred[i] == y_dt[i]:\n","    right = right +1\n","  #else:\n","  #  print (y_pred[i], y_dt[i])\n","  count = count + 1\n","ex = 2\n","#print (y_pred[ex], y_dt[ex])\n","print (right)\n","print (count)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.series.Series'>\n","(28000, 12482)\n","[1 1 1 ... 1 0 0]\n","8708\n","12000\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"]}]},{"cell_type":"code","metadata":{"id":"Pr9VtbUrxMCB"},"source":["#tensor conversion functions(work together on this on sunday)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aMVSEwugxsac"},"source":["#model function(RNN)(Vishal)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eErqo_OZx-p7"},"source":["#model function(LSTM)(Julian)\n","class LSTMBinaryClassifier(nn.Module):\n","  def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, weights_matrix):\n","    super(LSTMBinaryClassifier, self).__init__()\n","    self.hidden_dim = hidden_dim\n","    #weight matrix to tensor\n","    weights_matrix = torch.from_numpy(weights_matrix)\n","        \n","    num_embeddings, embedding_dim = weights_matrix.shape\n","    \n","    self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n","    #self.word_embeddings.load_state_dict({'weight': weights_matrix})\n","    #self.word_embeddings.weight.requires_grad = False\n","\n","    self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=False)\n","\n","    self.hidden_to_tag = nn.Linear(hidden_dim, tagset_size)\n","    #print('tagset size is:', tagset_size)\n","\n","  def forward(self, sentence):\n","    embeds = self.word_embeddings(sentence)\n","    lstm_out, (ht, ct) = self.lstm(embeds.view(len(sentence), 1, -1))\n","\n","    #print('length of lstm_out is', len(lstm_out))\n","    #print('length of sentence is', len(sentence))\n","    #print('the hidden dimension is', self.hidden_dim)\n","    #out_forward = lstm_out[range(len(lstm_out)), len(sentence) - 1, :self.hidden_dim]\n","\n","    #out_forward = lstm_out[:, -1, :self.hidden_dim]\n","\n","    #out_reverse = lstm_out[:, 0, self.dimension:]\n","    #out_reduced = torch.cat((out_forward, out_reverse), 1)\n","    #print(ht[-1])\n","    #print(ht.shape)\n","    tag_space = self.hidden_to_tag(ht[-1])\n","    #tag_space = torch.squeeze(tag_space, 1)\n","    #tag_scores = F.log_softmax(tag_space, dim=1)\n","    return tag_space"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9MvO-9H4yAoS"},"source":["#model function(biLSTM)(Julian)\n","class BiLSTMBinaryClassifier(nn.Module):\n","  def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, weights_matrix):\n","    super(BiLSTMBinaryClassifier, self).__init__()\n","    self.hidden_dim = hidden_dim\n","    #weight matrix to tensor\n","    weights_matrix = torch.from_numpy(weights_matrix)\n","        \n","    num_embeddings, embedding_dim = weights_matrix.shape\n","    \n","    self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n","    self.word_embeddings.load_state_dict({'weight': weights_matrix})\n","    self.word_embeddings.weight.requires_grad = False\n","\n","    self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n","\n","    self.hidden_to_tag = nn.Linear(2*hidden_dim, tagset_size)\n","  \n","  def forward(self, sentence):\n","    embeds = self.word_embeddings(sentence)\n","    lstm_out, (ht, ct) = self.lstm(embeds.view(len(sentence), 1, -1))\n","    out_forward = ht[-1]\n","    out_reverse = ht[0]\n","    out_reduced = torch.cat((out_forward, out_reverse), 1)\n","    #print(ht)\n","    #print(ht.shape)\n","    tag_space = self.hidden_to_tag(out_reduced)\n","    #tag_scores = F.log_softmax(tag_space, dim=1)\n","    return tag_space"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A-QzAuS8xOlS","executionInfo":{"status":"ok","timestamp":1638404701457,"user_tz":480,"elapsed":13,"user":{"displayName":"Julian Cremer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16604329281986994206"}},"outputId":"d8d2c5cb-5d8d-4a79-e4e5-d591c34112d3"},"source":["#training of models\n","#print(bla)\n","vocab = get_ix_converter(list(training_data['Sentences']))\n","data = training_data\n","tags = {'Intermediate': 0, 'Advanced': 1}\n","embedding_weights = glove_embedding(vocab.keys(), embeddings_dict)\n","\n","EMBEDDING_DIM = 100\n","HIDDEN_DIM = 100\n","#def train_bi_lstm(model, embedding_dim, hidden_dim, vocab_size, tagset_size, optimizer, loss_function):\n","#  print('Not Implemented')\n","\n","def train_lstm(model, loss_function, optimizer, training_data, epochs=10):\n","  for epoch in range(epochs):\n","    print(\"epoch\", epoch)\n","    for index, row in training_data.iterrows():\n","      model.zero_grad()\n","\n","      sentence_in = row['Sentences']\n","      targets = row['Tag']\n","      \n","      tag_scores = model(sentence_in)\n","      #print('targets:', targets)\n","      #print('scores:', tag_scores)\n","      #print (targets)\n","      loss = loss_function(tag_scores, targets)\n","      #print(loss)\n","      loss.backward()\n","      optimizer.step()\n","\n","def test_model(model, test_data_ix, test_data, word_to_ix, tag_to_ix):\n","  total = len(test_data)\n","  wrong = 0\n","  tags = list(tag_to_ix.keys())\n","  with torch.no_grad():\n","    for index, row in test_data_ix.iterrows():\n","      sentence_in = row['Sentences']\n","      #targets = row['IOB Slot tags']\n","\n","      tag_scores = model(sentence_in)\n","      #for scores in tag_scores:\n","      #print(tag_scores)\n","      scores = list(tag_scores[0])\n","      max_score = max(scores)\n","      #print('The maximum score is', max_score)\n","      tag = scores.index(max_score)\n","      predicted_tag = tags[tag]\n","      #print('Predicted tag: ', predicted_tag)\n","      #print('Actual Tag:', test_data.loc[index]['Tag'])\n","      #print(test_data.loc[index])\n","      if predicted_tag != test_data.loc[index]['Tag']:\n","        wrong += 1\n","        #print('Predicted Tags:', predicted_tags)\n","        #print('Actual Tags:', test_data.loc[index]['IOB Slot tags'])\n","    print('Accuracy: ', (total-wrong)/total)\n","\n","# input_data_ix = sentences converted to arrays of numbers\n","# tag_to_ix = dictionary with keys = tags and values equal to indexes\n","def run_model(model, input_data_ix, tag_to_ix):\n","  output = []\n","  tags = list(tag_to_ix.keys())\n","  with torch.no_grad():\n","    for index, row in input_data_ix.iterrows():\n","      sentence_in = row['Sentences']\n","      tag_scores = model(sentence_in)\n","      predicted_tags = []\n","      for scores in tag_scores:\n","        scores = list(scores)\n","        max_score = max(scores)\n","        tag = scores.index(max_score)\n","        predicted_tags.append(tags[tag])\n","      output.append(predicted_tags)\n","    #output = pd.DataFrame(output, orient='index',columns=['IOB Slot tags'])\n","    return output\n","\n","lstm = LSTMBinaryClassifier(EMBEDDING_DIM, HIDDEN_DIM, len(vocab), len(tags), embedding_weights)\n","bilstm = BiLSTMBinaryClassifier(EMBEDDING_DIM, HIDDEN_DIM, len(vocab), len(tags), embedding_weights)\n","loss_function = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(lstm.parameters(), lr=0.001)\n","bi_optimizer = optim.Adam(bilstm.parameters(), lr=0.001)\n","\n","training_data['Sentences'] = training_data['Sentences'].apply(lambda x: prepare_sequence(x, vocab))\n","training_data['Tag'] = training_data['Tag'].apply(lambda x: prepare_sequence([x], tags))\n","\n","train_ix = train.copy()\n","train_ix['Sentences'] = train_ix['Sentences'].apply(lambda x: prepare_sequence(x, vocab))\n","train_ix['Tag'] = train_ix['Tag'].apply(lambda x: prepare_sequence([x], tags))\n","dev_ix = dev.copy()\n","dev_ix['Sentences'] = dev_ix['Sentences'].apply(lambda x: prepare_sequence(x, vocab))\n","dev_ix['Tag'] = dev_ix['Tag'].apply(lambda x: prepare_sequence([x], tags))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(10940, 100)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EvRCdcjQEXl1","executionInfo":{"status":"ok","timestamp":1638405489270,"user_tz":480,"elapsed":787818,"user":{"displayName":"Julian Cremer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16604329281986994206"}},"outputId":"a3d7eb02-b8c4-403a-e3aa-bfe9969fef21"},"source":["#train_lstm(lstm, loss_function, optimizer, train_ix, epochs=20)\n","train_lstm(bilstm, loss_function, bi_optimizer, train_ix, epochs=20)\n","print (\"training done\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 0\n","epoch 1\n","epoch 2\n","epoch 3\n","epoch 4\n","epoch 5\n","epoch 6\n","epoch 7\n","epoch 8\n","epoch 9\n","epoch 10\n","epoch 11\n","epoch 12\n","epoch 13\n","epoch 14\n","epoch 15\n","epoch 16\n","epoch 17\n","epoch 18\n","epoch 19\n","training done\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3zBlDIMoRvjE","executionInfo":{"status":"ok","timestamp":1638405490270,"user_tz":480,"elapsed":1014,"user":{"displayName":"Julian Cremer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16604329281986994206"}},"outputId":"65c79d48-8dcd-42fd-d18e-02ad7491b38c"},"source":["#print (list(lstm.parameters()))\n","#run_model(lstm, training_data, tags)\n","#test_model(lstm, dev_ix, dev, vocab, tags)\n","test_model(bilstm, dev_ix, dev, vocab, tags)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy:  0.8654292343387471\n"]}]},{"cell_type":"markdown","metadata":{"id":"9Yd9f8FSxE90"},"source":["#Evaluation"]},{"cell_type":"code","metadata":{"id":"PvKUTsq6xdkg"},"source":["#run model on test data(Vishal)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v2FqWsKOxg5h"},"source":["#accuracy f1 precision recall metrics(Vishal)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3OyU9ZiszltO"},"source":["#Testing"]},{"cell_type":"markdown","metadata":{"id":"QnFOiDX8BHid"},"source":["add different things we want to test into their own code blocks"]},{"cell_type":"code","metadata":{"id":"2IJ62wKF0IK4"},"source":["#50 d embedding vs 100 d embedding GLOVe vs bert pretrained "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7BHFd6ap0Y_8"},"source":["#rnn vs lstm vs bilstm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rIjYt_m10bqb"},"source":["#different hyperparameter optimzization"],"execution_count":null,"outputs":[]}]}