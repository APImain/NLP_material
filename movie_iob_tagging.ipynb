{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"243hw3.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN/dGJHwE7W49TLZGZeNn/p"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"98sdWlMjOPQP"},"source":["sequence to sequence task"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"weQVNwyYOMh2","executionInfo":{"status":"ok","timestamp":1637802109511,"user_tz":480,"elapsed":6664,"user":{"displayName":"Karl Munson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15888825359313586957"}},"outputId":"94bf4455-431e-4fd2-a193-f210f2a3f97d"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import nltk\n","import csv\n","from sklearn.multiclass import OneVsRestClassifier\n","from sklearn.linear_model import SGDClassifier\n","import torch.nn.functional as F\n","torch.manual_seed(1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f28039db510>"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"V42G0FArPOui"},"source":["#my implementation\n","#imports in cell 1\n","#import data\n","embeddings_dict = {}\n","with open(\"glove.6B.50d.txt\", 'r', encoding=\"utf-8\") as f:\n","  for line in f:\n","    values = line.split()\n","    word = values[0]\n","    vector = np.asarray(values[1:], \"float32\")\n","    embeddings_dict[word] = vector\n","train_data = pd.read_csv(\"hw_3_train_data.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pyOVNplXQC39","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637802115642,"user_tz":480,"elapsed":5836,"user":{"displayName":"Karl Munson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15888825359313586957"}},"outputId":"ac019561-0213-48d3-cda2-300e528de97e"},"source":["x=[label.split() for label in train_data['utterances']]\n","#y = [label.split() for label in train_data['Core Relations']]\n","y = [label.split() for label in train_data['IOB Slot tags']]\n","\n","test_data = pd.read_csv(\"test_data.csv\")\n","x_test=[label.split() for label in test_data['utterances']]\n","\n","#get vocab\n","target_vocab = {\"<PAD>\":0}\n","index = 1\n","for sentence in x:\n","  for word in sentence:\n","    if word in target_vocab.keys():\n","      continue\n","    else:\n","      target_vocab[word] = index\n","      index = index +1\n","\n","for sentence in x_test:\n","  for word in sentence:\n","    if word in target_vocab.keys():\n","      continue\n","    else:\n","      target_vocab[word] = index\n","      index = index +1\n","\n","#embeddings\n","matrix_len = len(target_vocab)\n","weights_matrix = np.zeros((matrix_len, 50))\n","words_found = 0\n","\n","for i, word in enumerate(target_vocab):\n","    try: \n","        weights_matrix[i] = embeddings_dict[word]\n","        words_found += 1\n","    except KeyError:\n","       #continue\n","       weights_matrix[i] = np.random.normal(scale=0.6, size=(50, ))\n","\n","#labels\n","tag_vocab = {\"<PAD>\":0}\n","index = 1\n","for i in y:\n","  for tag_set in y:\n","    for tag in tag_set:\n","      if tag in tag_vocab.keys():\n","        continue\n","      else:\n","        tag_vocab[tag] = index\n","        index = index +1\n","#binarizer = MultiLabelBinarizer()\n","#binarizer.fit(y)\n","#y = binarizer.transform(y)\n","\n","\n","\n","x_train, x_dt, y_train, y_dt = train_test_split(x, y, test_size = .1, random_state = 0)\n","print ((weights_matrix.shape))\n","EMBEDDING_DIM = 50\n","HIDDEN_DIM = 50\n","print (tag_vocab)\n","print (len(target_vocab))\n","print (x_train[0], y_train[0])\n","print (embeddings_dict['word'])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(1482, 50)\n","{'<PAD>': 0, 'O': 1, 'B_char': 2, 'B_movie': 3, 'I_movie': 4, 'I_char': 5, 'B_person': 6, 'I_person': 7, 'B_director': 8, 'I_director': 9, 'B_release_year': 10, 'I_release_year': 11, 'B_location': 12, 'B_country': 13, 'I_country': 14, 'B_producer': 15, 'I_producer': 16, 'B_cast': 17, 'I_cast': 18, 'B_subject': 19, 'B_genre': 20, 'I-movie': 21, 'B_mpaa_rating': 22, 'I_mpaa_rating': 23, 'B_language': 24, 'I_language': 25, 'I_genre': 26, 'I_subject': 27}\n","1482\n","['what', 'movies', 'are', 'in', 'bengali'] ['O', 'O', 'O', 'O', 'B_language']\n","[-0.1643     0.15722   -0.55021   -0.3303     0.66463   -0.1152\n"," -0.2261    -0.23674   -0.86119    0.24319    0.074499   0.61081\n","  0.73683   -0.35224    0.61346    0.0050975 -0.62538   -0.0050458\n","  0.18392   -0.12214   -0.65973   -0.30673    0.35038    0.75805\n","  1.0183    -1.7424    -1.4277     0.38032    0.37713   -0.74941\n","  2.9401    -0.8097    -0.66901    0.23123   -0.073194  -0.13624\n","  0.24424   -1.0129    -0.24919   -0.06893    0.70231   -0.022177\n"," -0.64684    0.59599    0.027092   0.11203    0.61214    0.74339\n","  0.23572   -0.1369   ]\n"]}]},{"cell_type":"code","metadata":{"id":"Ed_EhyZUQYKE"},"source":["#padding of the sequences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TbOwpTltXTuu"},"source":["class LSTMTagger(nn.Module):\n","\n","    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, weights_matrix):\n","        super(LSTMTagger, self).__init__()\n","        self.hidden_dim = hidden_dim\n","\n","        #self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n","        #addtion\n","        weights_matrix = torch.from_numpy(weights_matrix)\n","        \n","        num_embeddings, embedding_dim = weights_matrix.shape\n","        self.word_embeddings = nn.Embedding(num_embeddings, embedding_dim)\n","        self.word_embeddings.load_state_dict({'weight': weights_matrix})\n","        self.word_embeddings.weight.requires_grad = False\n","\n","\n","        self.lstm = nn.LSTM(embedding_dim, embedding_dim, bidirectional = True)\n","\n","        self.hidden2tag = nn.Linear(embedding_dim * 2, tagset_size)\n","        #drop out\n","        self.dropout = torch.nn.Dropout(0.5)\n","\n","    def forward(self, sentence):\n","        embeds = self.word_embeddings(sentence)\n","        #embeds = self.dropout(embeds)\n","        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n","        #lstm_out = self.dropout(lstm_out)\n","        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n","        tag_scores = F.log_softmax(tag_space, dim=1)\n","        return tag_scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o8TxNXYbSEBG"},"source":["#model\n","def prepare_sequence(seq, to_ix, pad_length):\n","    idxs = [to_ix[w] for w in seq]\n","    tensor = torch.tensor(idxs, dtype=torch.long)\n","    if len(tensor) > pad_length:\n","      tensor = tensor[0:pad_length]\n","    if len(tensor) < pad_length:\n","      tensor = F.pad(input = tensor,pad=(0, pad_length-len(tensor)),  mode='constant', value = 0)\n","    return tensor\n","\n","model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(target_vocab), len(tag_vocab), weights_matrix)\n","loss_function = nn.NLLLoss()\n","#loss_function = nn.CrossEntropyLoss()\n","#\n","optimizer = optim.SGD(model.parameters(), lr=0.1)\n","#optimizer = optim.Adam(model.parameters(), lr = 0.01 )\n","\n","\n","#previously tried 8 12 16 21\n","pad_length = 22\n","\n","\n","for epoch in range(100): \n","    print (\"epoch:\", epoch)\n","    sentences = []\n","    targets = []\n","    for i in range(0, len(x_train)):\n","        sentences.append(prepare_sequence(x_train[i], target_vocab, pad_length))\n","        #print (y_train.to_list())\n","        targets.append(prepare_sequence(y_train[i], tag_vocab, pad_length))\n","\n","    for i  in range(0, len(x_train)):\n","\n","        model.zero_grad()\n","\n","        sentence_in = sentences[i]\n","        target = targets[i]\n","\n","        tag_scores = model(sentence_in)\n","\n","        loss = loss_function(tag_scores, target)\n","        loss.backward()\n","        optimizer.step()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NQD_nLEX_QQq"},"source":["#testing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HenLsV7vk4_U","executionInfo":{"status":"ok","timestamp":1637812744055,"user_tz":480,"elapsed":4008,"user":{"displayName":"Karl Munson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15888825359313586957"}},"outputId":"f5abfe20-7742-4b9e-c443-2caa57f3b50b"},"source":["y_pred = []\n","#x_dt\n","x_val = x_train\n","y_val = y_train\n","count = 0\n","right = 0\n","with torch.no_grad():\n","  for i in range(0, len(y_val)):\n","    pred = ((model(prepare_sequence(x_val[i], target_vocab, pad_length)).numpy())) #np.argmax\n","    temp= []\n","    for row in pred:\n","      best_tag = np.argmax(row)\n","      best_tag = [key for key, value in tag_vocab.items() if value == best_tag]\n","      temp.append(best_tag[0])\n","    #print (temp)\n","    pred = temp\n","    y_pred.append(pred)\n","    #print (y_pred[i])\n","  for i in range(0,len(y_pred)):\n","    if len(y_pred[i]) > len(y_val[i]):\n","      y_pred[i] = y_pred[i][0:len(y_val[i])]\n","    if y_pred[i] == ():\n","      y_pred[i] = ('none',)\n","    if y_pred[i] == y_val[i]:\n","      right = right +1\n","    #else:\n","    #print (y_pred[i], y_val[i])\n","    count = count + 1\n","    #if len(y_pred[i]) != len(y_val[i]):\n","      #print (i, y_pred[i], y_val[i])\n","  #ex = 3\n","  #print (y_pred[ex], y_val[ex])\n","  print (right)\n","  print (count)\n","  print (right/count)\n","\n","y_pred = []\n","x_val = x_dt\n","y_val = y_dt\n","count = 0\n","right = 0\n","with torch.no_grad():\n","  for i in range(0, len(y_val)):\n","    pred = ((model(prepare_sequence(x_val[i], target_vocab, pad_length)).numpy())) #np.argmax\n","    temp= []\n","    for row in pred:\n","      best_tag = np.argmax(row)\n","      best_tag = [key for key, value in tag_vocab.items() if value == best_tag]\n","      temp.append(best_tag[0])\n","    #print (temp)\n","    pred = temp\n","    y_pred.append(pred)\n","    #print (y_pred[i])\n","  for i in range(0,len(y_pred)):\n","    if len(y_pred[i]) > len(y_val[i]):\n","      y_pred[i] = y_pred[i][0:len(y_val[i])]\n","    if y_pred[i] == ():\n","      y_pred[i] = ('none',)\n","    if y_pred[i] == y_val[i]:\n","      right = right +1\n","    #else:\n","    #print (y_pred[i], y_val[i])\n","    count = count + 1\n","    #if len(y_pred[i]) != len(y_val[i]):\n","      #print (i, y_pred[i], y_val[i])\n","  #ex = 3\n","  #print (y_pred[ex], y_val[ex])\n","  print (right)\n","  print (count)\n","  print (right/count)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023\n","2027\n","0.9980266403552047\n","177\n","226\n","0.7831858407079646\n"]}]},{"cell_type":"code","metadata":{"id":"K2T-7fh2E9ET"},"source":["#exporting test data\n","import csv\n","y_pred = []\n","x_val = x_test\n","with torch.no_grad():\n","  for i in range(0, len(x_val)):\n","    pred = ((model(prepare_sequence(x_val[i], target_vocab, pad_length)).numpy())) #np.argmax\n","    temp= []\n","    for row in pred:\n","      best_tag = np.argmax(row)\n","      best_tag = [key for key, value in tag_vocab.items() if value == best_tag]\n","      temp.append(best_tag[0])\n","    #print (temp)\n","    pred = temp\n","    y_pred.append(pred)\n","    #print (y_pred[i])\n","\n","with open('model_output.csv', 'w', newline = '') as csvfile:\n","    writer = csv.writer(csvfile, delimiter = ',', lineterminator='\\n')\n","    writer.writerow(['Id', 'Predicted'])\n","    count = 0\n","    for i in range(0,len(y_pred)):\n","      y_pred[i] = y_pred[i][0 : len(x_test[i])]\n","      for j in y_pred[i]:\n","        out = j\n","        writer.writerow((count, out))\n","        count += 1\n","    #buffering size until error is found\n","    #writer.writerow((count, out))"],"execution_count":null,"outputs":[]}]}