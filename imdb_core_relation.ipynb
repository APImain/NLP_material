{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"243hw2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPg5VTOnBBfJgkF158D+kU1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ZQguRKirHh_g","executionInfo":{"status":"ok","timestamp":1636392505075,"user_tz":480,"elapsed":26427,"user":{"displayName":"Karl Munson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15888825359313586957"}}},"source":["#imports\n","import pandas as pd\n","import torch\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import nltk\n","import csv\n","from sklearn.multiclass import OneVsRestClassifier\n","from sklearn.linear_model import SGDClassifier\n","import torch.nn.functional as F"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0RBZZK6KK-Cx"},"source":["#Provided Eval Metrics"]},{"cell_type":"markdown","metadata":{"id":"0sw3l3CDLFJs"},"source":["#Preprocessing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":189},"id":"EPVcBoGV4mRj","executionInfo":{"status":"ok","timestamp":1636399992925,"user_tz":480,"elapsed":482,"user":{"displayName":"Karl Munson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15888825359313586957"}},"outputId":"bf487bf1-bc87-4af3-edea-9a983ca04d2e"},"source":["\n","# data load with tfidf\n","train_data = pd.read_csv(\"train_data.csv\")\n","\n","x=train_data['utterances']\n","y = [label.split() for label in train_data['Core Relations']]\n","\n","binarizer = MultiLabelBinarizer()\n","binarizer.fit(y)\n","y = binarizer.transform(y)\n","\n","x_train, x_dt, y_train, y_dt = train_test_split(x, y, test_size = .1, random_state = 0)\n","#x_dev, x_test, y_dev, y_test = train_test_split(x_dt, y_dt, test_dt = .5, random_state = 0)\n","tfidf = TfidfVectorizer(min_df = 0)\n","fitted = tfidf.fit(x_train.append(x_dt))\n","x_train = fitted.transform(x_train)\n","x_dt = fitted.transform(x_dt)\n","print(x_train.shape)\n","train_data.describe()"],"execution_count":130,"outputs":[{"output_type":"stream","name":"stdout","text":["(1577, 1117)\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>utterances</th>\n","      <th>Core Relations</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>2253</td>\n","      <td>2253</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>2163</td>\n","      <td>47</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>who directed finding nemo</td>\n","      <td>movie.directed_by</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>6</td>\n","      <td>314</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                       utterances     Core Relations\n","count                        2253               2253\n","unique                       2163                 47\n","top     who directed finding nemo  movie.directed_by\n","freq                            6                314"]},"metadata":{},"execution_count":130}]},{"cell_type":"code","metadata":{"id":"qQJMuqXzCU02","executionInfo":{"status":"ok","timestamp":1636392508563,"user_tz":480,"elapsed":2990,"user":{"displayName":"Karl Munson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15888825359313586957"}}},"source":["#glove download\n","embeddings_dict = {}\n","with open(\"glove.6B.50d.txt\", 'r', encoding=\"utf-8\") as f:\n","  for line in f:\n","    values = line.split()\n","    word = values[0]\n","    vector = np.asarray(values[1:], \"float32\")\n","    embeddings_dict[word] = vector\n","train_data = pd.read_csv(\"train_data.csv\")\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WqUv7U-YzMp6","executionInfo":{"status":"ok","timestamp":1636398927512,"user_tz":480,"elapsed":1540,"user":{"displayName":"Karl Munson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15888825359313586957"}},"outputId":"89b165b5-a476-402f-9100-7c4fafba1d8c"},"source":["#data load with word embedding and padding\n","\n","#def find_closest_embeddings(embedding):\n","#    return sorted(embeddings_dict.keys(), key=lambda word: spatial.distance.euclidean(embeddings_dict[word], embedding))\n","\n","\n","x=[label.split() for label in train_data['utterances']]\n","#y = [label.split() for label in train_data['Core Relations']]\n","y = train_data['Core Relations']\n","#get vocab\n","target_vocab = {}\n","index = 0\n","for sentence in x:\n","  for word in sentence:\n","    if word in target_vocab.keys():\n","      continue\n","    else:\n","      target_vocab[word] = index\n","      index = index +1\n","\n","#embeddings\n","matrix_len = len(target_vocab)\n","weights_matrix = np.zeros((matrix_len, 50))\n","words_found = 0\n","\n","for i, word in enumerate(target_vocab):\n","    try: \n","        weights_matrix[i] = embeddings_dict[word]\n","        words_found += 1\n","    except KeyError:\n","       continue\n","        #weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))\n","\n","#labels\n","tag_vocab = {}\n","index = 0\n","for i in y:\n","  for word in y:\n","    if word in tag_vocab.keys():\n","      continue\n","    else:\n","      tag_vocab[word] = index\n","      index = index +1\n","#binarizer = MultiLabelBinarizer()\n","#binarizer.fit(y)\n","#y = binarizer.transform(y)\n","\n","\n","\n","x_train, x_dt, y_train, y_dt = train_test_split(x, y, test_size = .1, random_state = 0)\n","print (type(weights_matrix))\n","EMBEDDING_DIM = 50\n","HIDDEN_DIM = 50\n","print (tag_vocab)\n","print (len(tag_vocab))\n","print (x_train[0], y_train[0])\n"],"execution_count":113,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'numpy.ndarray'>\n","{'movie.starring.actor movie.starring.character': 0, 'movie.starring.actor': 1, 'movie.starring.actor actor.gender': 2, 'person.date_of_birth': 3, 'movie.estimated_budget': 4, 'movie.estimated_budget movie.directed_by': 5, 'movie.starring.character': 6, 'movie.music': 7, 'movie.initial_release_date': 8, 'movie.locations': 9, 'movie.locations movie.directed_by': 10, 'movie.produced_by': 11, 'movie.production_companies': 12, 'movie.directed_by': 13, 'movie.starring.actor movie.directed_by': 14, 'movie.country': 15, 'movie.directed_by movie.initial_release_date': 16, 'movie.production_companies movie.initial_release_date': 17, 'movie.starring.actor movie.initial_release_date': 18, 'movie.directed_by movie.produced_by': 19, 'movie.directed_by movie.subjects': 20, 'movie.directed_by movie.genre': 21, 'movie.genre': 22, 'movie.production_companies movie.genre': 23, 'movie.language': 24, 'none': 25, 'movie.country movie.language movie.genre': 26, 'movie.country movie.language': 27, 'movie.rating': 28, 'movie.rating movie.genre': 29, 'movie.subjects': 30, 'movie.gross_revenue': 31, 'movie.gross_revenue movie.genre': 32, 'movie.language movie.genre': 33, 'movie.country movie.genre': 34, 'movie.country movie.language movie.genre movie.rating': 35, 'movie.country movie.locations': 36, 'movie.rating movie.directed_by': 37, 'movie.rating movie.starring.actor': 38, 'movie.genre movie.subjects': 39, 'movie.rating movie.production_companies': 40, 'movie.initial_release_date movie.produced_by': 41, 'movie.rating movie.initial_release_date': 42, 'movie.gross_revenue gr.amount': 43, 'movie.genre movie.initial_release_date': 44, 'movie.production_companies movie.produced_by': 45, 'movie.country movie.rating': 46}\n","47\n","['show', 'me', 'the', 'movies', 'that', 'spielberg', 'made'] movie.starring.actor movie.starring.character\n"]}]},{"cell_type":"markdown","metadata":{"id":"1KEIYSaGKyqa"},"source":["#Models"]},{"cell_type":"code","metadata":{"id":"qCuYE_gV807W"},"source":["#single direction lstm\n","#for tfidf (1577, 1117)\n","def prepare_sequence(seq, to_ix):\n","    #return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]\n","    idxs = [to_ix[w] for w in seq]\n","    return torch.tensor(idxs, dtype=torch.long)\n","def prepare_sequenceT(seq, to_ix):\n","    #return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]\n","    idxs = to_ix[seq]\n","    #print (seq)\n","    #print (idxs)\n","    #print (to_ix)\n","    return torch.tensor([idxs], dtype=torch.long)\n","\n","def create_emb_layer(weights_matrix, non_trainable=False):\n","    #modified check numbers are right\n","    num_embeddings= len(weights_matrix)\n","    embedding_dim = len(weights_matrix[1])\n","\n","    emb_layer = torch.nn.Embedding(num_embeddings, embedding_dim)\n","    #important step that doesnt work fix this\n","    weights_matrix = torch.tensor(weights_matrix, dtype=torch.long)\n","    emb_layer.load_state_dict({'weight': weights_matrix})\n","    if non_trainable:\n","        emb_layer.weight.requires_grad = False\n","\n","    return emb_layer, num_embeddings, embedding_dim\n","\n","class LSTMTagger(torch.nn.Module):\n","\n","    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, weights_matrix):\n","        super(LSTMTagger, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        #modify this\n","        #self.embeddings, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n","        #self.embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n","        self.embeddings = torch.nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n","        self.embeddings.weight.data.copy_(torch.from_numpy(weights_matrix))\n","        self.embeddings.weight.requires_grad = False\n","        self.dropout = torch.nn.Dropout(0.3)\n","        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n","        self.linear = torch.nn.Linear(hidden_dim, tagset_size)\n","\n","    def forward(self, sentence, s):\n","        x = self.embeddings(sentence)\n","        print (x.shape)\n","        #switch point for an RNN or GRU\n","        #self.RNN(x)\n","        #self.GRU(x.view(len(sentence), 1, -1))\n","        lstm_out, (ht, ct) = self.lstm(x.view(len(sentence), 1, -1))\n","        print (lstm_out.shape)\n","        lin =self.linear(ht[-1])\n","        print(lin.shape)\n","        #tag_scores = F.log_softmax(lin)\n","        return lin\n","\n","\n","\n","model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(target_vocab), len(tag_vocab), weights_matrix)\n","loss_function = F.cross_entropy\n","#torch.nn.MSELoss\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","#optimizer = torch.optim.Adam([var1, var2], lr=0.0001)\n","#adagrad RMSprop implementing SGD vs Adam this time\n","\n","for epoch in range(100):  \n","    sentences = []\n","    targets = []\n","    for i in range(0, len(x_train)):\n","        sentences.append(prepare_sequence(x_train[i], target_vocab))\n","        #print (y_train.to_list())\n","        targets.append(prepare_sequenceT(y_train.to_list()[i], tag_vocab))\n","        #targets.append(torch.FloatTensor((y_train)))\n","    torch.nn.utils.rnn.pad_sequence(sentences)\n","    #torch.nn.utils.rnn.pad_sequence(targets)\n","    for i  in range(0, len(x_train)):\n","        #  clear gradients aparently\n","        model.zero_grad()\n","        #tensors\n","        sentence_in = sentences[i]\n","        target = targets[i]\n","        #print (targets)\n","        # Step 3. Run our forward pass.\n","        tag_scores = model(sentence_in, 40)\n","\n","        #computing loss\n","        #print (len(tag_scores))\n","\n","        print(tag_scores)\n","        print (target)\n","        loss = loss_function(tag_scores, target)\n","        loss.backward()\n","        optimizer.step()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YSPnt0IxH6Wb"},"source":["#verifying lstm results\n","y_pred = []\n","count = 0\n","right = 0\n","with torch.no_grad():\n","  for i in range(0, len(y_dt)):\n","    pred = (np.argmax(model(prepare_sequence(x_train[i], target_vocab), 40).numpy()))\n","    y_pred.append([key for key, value in tag_vocab.items() if value == pred]\n",")\n","    print (y_pred[i])\n","  for i in range(0,len(y_pred)):\n","    if y_pred[i] == ():\n","      y_pred[i] = ('none',)\n","    if y_pred[i] == y_dt[i]:\n","      right = right +1\n","    #else:\n","    #  print (y_pred[i], y_dt[i])\n","    count = count + 1\n","  ex = 3\n","  print (y_pred[ex], y_dt[ex])\n","  print (right)\n","  print (count)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pmqSVpY6JTmZ","executionInfo":{"status":"ok","timestamp":1636400004259,"user_tz":480,"elapsed":163,"user":{"displayName":"Karl Munson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15888825359313586957"}},"outputId":"6eda6954-15c5-44e1-aa80-14706daeebf5"},"source":["#default approach \n","classifier = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha =.0001))\n","classifier.fit(x_train, y_train)\n","\n","\n","y_pred = classifier.predict(x_dt)\n","count = 0\n","right = 0\n","y_pred = binarizer.inverse_transform(y_pred)\n","y_dt = binarizer.inverse_transform(y_dt)\n","for i in range(0,len(y_pred)):\n","  if y_pred[i] == ():\n","    y_pred[i] = ('none',)\n","  if y_pred[i] == y_dt[i]:\n","    right = right +1\n","  #else:\n","  #  print (y_pred[i], y_dt[i])\n","  count = count + 1\n","ex = 2\n","print (y_pred[ex], y_dt[ex])\n","print (right)\n","print (count)"],"execution_count":131,"outputs":[{"output_type":"stream","name":"stdout","text":["('movie.directed_by',) ('movie.directed_by',)\n","539\n","676\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B5EQtkV-THhZ","executionInfo":{"status":"ok","timestamp":1636400006781,"user_tz":480,"elapsed":152,"user":{"displayName":"Karl Munson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15888825359313586957"}},"outputId":"6db4699f-e6ad-4c30-b001-497e514bf9b0"},"source":["ex = 423\n","print (y_pred[ex], y_dt[ex])"],"execution_count":132,"outputs":[{"output_type":"stream","name":"stdout","text":["('movie.initial_release_date', 'movie.production_companies') ('movie.initial_release_date', 'movie.production_companies')\n"]}]}]}